# %%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# %%
X, y = fetch_california_housing(return_X_y=True)
feature_names = [
    "MedInc", "HouseAge", "AveRooms", "AveBedrms",
    "Population", "AveOccup", "Latitude", "Longitude"
]

# %%
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=302
)

# %%
X_train_split,X_val_split,y_train_split,y_val_split=train_test_split(
    X_train,y_train,test_size=0.20,random_state=302
)

# %%
df_train = pd.DataFrame(X_train_split,columns=feature_names)
print(df_train)


# %%
# here is a funcation show out the 8 features and there characteristics representation 
plt.figure(figsize=(16,12))
for i, feature in enumerate(feature_names):
    plt.subplot(4,2,i+1)
    plt.hist(df_train[feature],bins=40,edgecolor='black')
    plt.title(f"Distribution of {feature}")
    plt.xlabel(feature)
    plt.ylabel("Count")
plt.tight_layout()
plt.show()


# %%
print(df_train["MedInc"].head())

# %%
#Standard_ scaler changes every feature so that:
#its mean becomes 0
#its standard deviation becomes 1
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_split)
X_val_scaled = scaler.transform(X_val_split)
X_test_scaled = scaler.transform(X_test)
print("Training shape:", X_train_scaled.shape)
print("Validation shape:", X_val_scaled.shape)
print("Test shape:", X_test_scaled.shape)

# %%
#we convert numpy here to tensor so pytorch easy to works with tensors 
X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_t = torch.tensor(y_train_split, dtype=torch.float32).view(-1, 1)
X_val_t = torch.tensor(X_val_scaled, dtype=torch.float32)
y_val_t = torch.tensor(y_val_split, dtype=torch.float32).view(-1, 1)
#we here load the data 
batch_size = 64 
train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True)
val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=batch_size)

# %%
# here a main class contain the how many units i will use and the hidden layers& units also 
class FeedforwardNet(nn.Module):
    def __init__(self, input_dim, hidden_layers, hidden_units):
        super().__init__()
        layers = []
        in_dim = input_dim
        for _ in range(hidden_layers):
            layers.append(nn.Linear(in_dim, hidden_units))
            layers.append(nn.ReLU())
            in_dim = hidden_units
        layers.append(nn.Linear(in_dim, 1))
        self.net = nn.Sequential(*layers)
    def forward(self, x):
        return self.net(x)

# %%
# here i put the model in the training mode with the learning rate 0.01 and 50 Epoches
# train the model use MSELoss ,use learning rate changeable , calculate the training loss and validation loss 
# also epochs have ability to change it 
# return last train loss and validation lodd  
def train_model(model, train_loader, val_loader, lr=0.01, epochs=50):
    criterion = nn.MSELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for xb, yb in train_loader:
            optimizer.zero_grad()
            pred = model(xb)
            loss = criterion(pred, yb)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * xb.size(0)
        train_loss /= len(train_loader.dataset)
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for xb, yb in val_loader:
                pred = model(xb)
                loss = criterion(pred, yb)
                val_loss += loss.item() * xb.size(0)
        val_loss /= len(val_loader.dataset)
    return train_loss, val_loss

# %%
## use many architectures
# same number of epochs
# same learning rate
# calculate train loss and val loss then print the output in a pandas table
import pandas as pd
results = []
architectures = [
    (1, 16), (1, 32), (1, 64),
    (2, 32), (2, 64), (2, 128),
    (3, 32), (3, 64), (3, 128),
    (4, 64)
]
for hidden_layers, hidden_units in architectures:
    model = FeedforwardNet(8, hidden_layers, hidden_units)
    train_loss, val_loss = train_model(
        model,
        train_loader,
        val_loader,
        lr=0.01,      
        epochs=50
    )
    results.append({
        "layers": hidden_layers,
        "units": hidden_units,
        "lr": 0.01,
        "epochs": 50,
        "train_loss": train_loss,
        "val_loss": val_loss
    })

df_results = pd.DataFrame(results)
print(df_results)

# %%
best_row = df_results.iloc[df_results['val_loss'].idxmin()]
best_layers = int(best_row['layers'])
best_units = int(best_row['units'])
print(f"\n Best Architecture: {best_layers} hidden layers  {best_units} units")

# %%
# improved model from last training function
# with other feature
# save all history for train_loss and validation loss
# print the loss every 10 epochs
def train_model_with_history(model, train_loader, val_loader, lr=0.01, epochs=100):
    criterion = nn.MSELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=lr)
    train_losses = []
    val_losses = []
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        for xb, yb in train_loader:
            optimizer.zero_grad()
            pred = model(xb)
            loss = criterion(pred, yb)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * xb.size(0)
        train_loss /= len(train_loader.dataset)
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for xb, yb in val_loader:
                pred = model(xb)
                loss = criterion(pred, yb)
                val_loss += loss.item() * xb.size(0)
        val_loss /= len(val_loader.dataset)
        train_losses.append(train_loss)
        val_losses.append(val_loss)
    return train_losses, val_losses

# %%
print(train_loss)
print(val_loss)

# %%
best_model = FeedforwardNet(input_dim=8, hidden_layers=best_layers, hidden_units=best_units)
train_losses, val_losses = train_model_with_history(
    best_model,
    train_loader,
    val_loader,
    lr=0.01,
    epochs=100
)
plt.figure(figsize=(7,5))
plt.plot(train_losses, label="Training Loss")
plt.plot(val_losses, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.title("Training & Validation Loss Evolution")
plt.legend()
plt.grid(True)
plt.show()


# %%
# Create the final model using the best architecture
# Train the model on the full training dataset (train + validation)
final_model = FeedforwardNet(8, best_layers, best_units)
final_train_losses, _ = train_model_with_history(
    final_model,
    full_loader,
    val_loader,
    lr=0.01,
    epochs=100
)
X_full_train = np.concatenate([X_train_scaled,X_val_scaled], axis=0)
y_full_train = np.concatenate([y_train_split,y_val_split], axis=0)
X_full_t = torch.tensor(X_full_train, dtype=torch.float32)
y_full_t = torch.tensor(y_full_train, dtype=torch.float32).view(-1,1)
full_loader = DataLoader(TensorDataset(X_full_t, y_full_t),batch_size=batch_size,shuffle=True)
X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1,1)
test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=batch_size)
final_model.eval()
criterion = nn.MSELoss()
test_loss = 0.0
predictions = []
with torch.no_grad():
    for xb, yb in test_loader:
        preds = final_model(xb)
        predictions.append(preds)
        test_loss += criterion(preds, yb).item() * xb.size(0)

test_loss /= len(test_loader.dataset)
predictions = torch.cat(predictions).numpy().flatten()
print("Final Test MSE:", test_loss)
plt.figure(figsize=(6,6))
plt.scatter(predictions, y_test, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel("Predicted Median House Value")
plt.ylabel("Actual Median House Value")
plt.title("Predicted vs Actual Prices on Test Set")
plt.grid(True)
plt.show()


# %%
# If the house value is less than 2  it becomes 0
# else it becomes 1
y_train_bin = np.where(y_train < 2, 0, 1)
y_val_bin = np.where(y_val_split < 2, 0, 1)
y_test_bin = np.where(y_test < 2, 0, 1)

# %%
# Convert the binary values (0/1) into tensors
# Reshape them to the correct form (N, 1) using .view(-1, 1)
# Use dtype = float32 
y_train_bin = np.where(y_train_split < 2, 0, 1)
y_val_bin   = np.where(y_val_split < 2, 0, 1)
y_test_bin  = np.where(y_test < 2, 0, 1)

# %%
# Reused the same architecture
# The last Linear layer has 1 output, followed by a Sigmoid() to map values between 0 and 1
# The forward function is simple and clean
class FeedforwardBinary(nn.Module):
    def __init__(self, input_dim, hidden_layers, hidden_units):
        super().__init__()
        layers = []
        in_dim = input_dim
        for _ in range(hidden_layers):
            layers.append(nn.Linear(in_dim, hidden_units))
            layers.append(nn.ReLU())
            in_dim = hidden_units
        layers.append(nn.Linear(in_dim, 1))
        layers.append(nn.Sigmoid()) 
        self.net = nn.Sequential(*layers)
    def forward(self, x):
        return self.net(x)

# %%
# convert inputs and binary labels into tensors
# create dataloader for binary classification
# define and train binary model
X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)
X_val_t   = torch.tensor(X_val_scaled,   dtype=torch.float32)
X_test_t  = torch.tensor(X_test_scaled,  dtype=torch.float32)
y_train_t = torch.tensor(y_train_bin, dtype=torch.float32).view(-1, 1)
y_val_t   = torch.tensor(y_val_bin,   dtype=torch.float32).view(-1, 1)
y_test_t  = torch.tensor(y_test_bin,  dtype=torch.float32).view(-1, 1)
train_loader_bin = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True)
val_loader_bin   = DataLoader(TensorDataset(X_val_t,   y_val_t),   batch_size=batch_size, shuffle=False)
test_loader_bin  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=batch_size, shuffle=False)
model_bin = FeedforwardBinary(8, best_layers, best_units)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model_bin.parameters(), lr=0.001)
epochs = 50
for epoch in range(epochs):
    model_bin.train()
    total_loss = 0
    for xb, yb in train_loader_bin:
        optimizer.zero_grad()
        preds = model_bin(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * xb.size(0)
    total_loss /= len(train_loader_bin.dataset)
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs} - Loss: {total_loss:.4f}")

# %%
model_bin.eval()
predictions = []
with torch.no_grad():
    for xb, _ in test_loader_bin:
        preds = model_bin(xb)
        predictions.append(preds)
predictions = torch.cat(predictions).numpy().flatten()
predicted_classes = (predictions >= 0.5).astype(int)

# %%
# in this cell we here calculated the accuracy,precision,recall, F1 score  
accuracy = accuracy_score(y_test_bin, predicted_classes)
precision = precision_score(y_test_bin, predicted_classes)
recall = recall_score(y_test_bin, predicted_classes)
f1 = f1_score(y_test_bin, predicted_classes)
print(f"Accuracy:  {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall:    {recall:.3f}")
print(f"F1 Score:  {f1:.3f}")


